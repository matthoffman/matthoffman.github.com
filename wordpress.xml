<?xml version="1.0" encoding="UTF-8"?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your blog. -->
<!-- It contains information about your blog's posts, comments, and categories. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your blog. -->

<!-- To import this information into a WordPress blog follow these steps. -->
<!-- 1. Log in to that blog as an administrator. -->
<!-- 2. Go to Tools: Import in the blog's admin panels (or Manage: Import in older versions of WordPress). -->
<!-- 3. Choose "WordPress" from the list. -->
<!-- 4. Upload this file using the form provided on that page. -->
<!-- 5. You will first be asked to map the authors in this export file to users -->
<!--    on the blog.  For each author, you may choose to map to an -->
<!--    existing user on the blog or to create a new user -->
<!-- 6. WordPress will then import each of the posts, comments, and categories -->
<!--    contained in this file into your blog -->

<!-- generator="WordPress.com" created="2011-01-31 02:28"-->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.0/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.0/"
>

<channel>
	<title>matt thinks so</title>
	<link>http://mattthinksso.wordpress.com</link>
	<description></description>
	<pubDate>Sun, 30 Jan 2011 02:05:31 +0000</pubDate>
	<generator>http://wordpress.org/?v=MU</generator>
	<language>en</language>
	<wp:wxr_version>1.0</wp:wxr_version>
	<wp:base_site_url>http://wordpress.com/</wp:base_site_url>
	<wp:base_blog_url>http://mattthinksso.wordpress.com</wp:base_blog_url>
		<wp:category><wp:category_nicename>programming</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Programming]]></wp:cat_name></wp:category>
		<wp:category><wp:category_nicename>uncategorized</wp:category_nicename><wp:category_parent></wp:category_parent><wp:cat_name><![CDATA[Uncategorized]]></wp:cat_name></wp:category>
		<wp:category><wp:category_nicename>big-data</wp:category_nicename><wp:category_parent>Programming</wp:category_parent><wp:cat_name><![CDATA[Big Data]]></wp:cat_name></wp:category>
			
	<generator>http://wordpress.com/</generator>
<cloud domain='mattthinksso.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>http://s2.wp.com/i/buttonw-com.png</url>
		<title>matt thinks so</title>
		<link>http://mattthinksso.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="http://mattthinksso.wordpress.com/osd.xml" title="matt thinks so" />
	<atom:link rel='hub' href='http://mattthinksso.wordpress.com/?pushpress=hub'/>

		<item>
		<title>where we&#039;re at</title>
		<link>http://mattthinksso.wordpress.com/?p=20</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=20</guid>
		<description></description>
		<content:encoded><![CDATA[Like I said previously, we're at the cusp of big data.  Our Oracle database is about a terabyte on disk, but most of the data we process daily is a in the tens of gigabytes.  That translates into tables that range from a few million rows to a few hundred million rows.  Our workload is a combination of batch and OLTP, but I'm most concerned about batch when I'm doing architectural sketches; Oracle can handle our OLTP needs for a while yet.  But if a new architecture addresses both, so much the better.   ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>20</wp:post_id>
		<wp:post_date>2009-12-19 14:16:42</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1296337813:11329143]]></wp:meta_value>
		</wp:postmeta>
							</item>
		<item>
		<title>Auto Draft</title>
		<link>http://mattthinksso.wordpress.com/?p=26</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=26</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>26</wp:post_id>
		<wp:post_date>2011-01-25 09:16:47</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>auto-draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
							</item>
		<item>
		<title>About</title>
		<link>http://mattthinksso.wordpress.com/about/</link>
		<pubDate>Sat, 19 Dec 2009 03:52:19 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Uncategorized]]></category>

		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false"></guid>
		<description></description>
		<content:encoded><![CDATA[(fill in "about me" here)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2</wp:post_id>
		<wp:post_date>2009-12-19 03:52:19</wp:post_date>
		<wp:post_date_gmt>2009-12-19 03:52:19</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>about</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>page</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1261195847]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_wp_page_template</wp:meta_key>
		<wp:meta_value><![CDATA[default]]></wp:meta_value>
		</wp:postmeta>
							</item>
		<item>
		<title>on the cusp of Big Data</title>
		<link>http://mattthinksso.wordpress.com/2009/12/19/on-the-cusp-of-big-data/</link>
		<pubDate>Sat, 19 Dec 2009 19:12:35 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Big Data]]></category>

		<category domain="category" nicename="big-data"><![CDATA[Big Data]]></category>

		<category><![CDATA[Programming]]></category>

		<category domain="category" nicename="programming"><![CDATA[Programming]]></category>

		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=6</guid>
		<description></description>
		<content:encoded><![CDATA[<i>Alternate title:</i> <b>Episode IV: A New Hope</b>

I'm restarting this blog after a long hiatus -- a couple of years, at least.  It looks like my old posts were purged in the meantime, but that's probably for the best<a name="to1" href="#1"><sup>1</sup></a>.  My musings about Hibernate from 2007 are probably not that interesting now.

Where I'm coming from:

I work on a team that, in a lot of ways, is on the cusp of Big Data:  we deal with hundreds of gigabytes, but not terabytes of data, and we don't have endless racks of commodity servers.  We have a homegrown task framework that follows a typical master-worker pattern and allows for tasks to be distributed among nodes on different servers.  That works well, and I like the framework in general -- it could use some cleaning up, but it's simple, clean and functional.

Data, though, is all stored inside an Oracle database, and we're knocking at the edge of it's capabilities.  We haven't entirely maxed it out yet, but each performance gain has been harder to come by, and we can easily see the time approaching where it will be cheaper to rearchitect how we're storing and serving data rather than eke more performance by smarter partitioning, better queries, or a faster SAN.

So over the past several months I've been reading about some of the competitors in the big-data field, and sketching ideas for what I'd like our architecture to look like going forward.  Things like MapReduce (Hadoop), HBase, Cassandra, or Terracotta, or a number of other ideas -- different types of products, all with the goal of scaling data beyond a single server.  But unlike a lot of folks looking at these options, we have an existing product in production, based on a framework that does 80% of what we need.  So I find myself on a seesaw, going between the newest, coolest thing I've read about, and then the pain of rewriting what we have on a non-existent timeline when what we have works so well -- at today's data volumes.

I decided to reinstate this blog to collect what I've learned so far.

<a name="1" href="#to1"><sup>1</sup></a>does anyone else have the problem of starting blogs like new years' resolutions and then losing track of them?  I probably have three out there, tied to some forgotten username on goodness knows which host, and it's probably saying very insightful things about Hibernate 3.0.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6</wp:post_id>
		<wp:post_date>2009-12-19 14:12:35</wp:post_date>
		<wp:post_date_gmt>2009-12-19 19:12:35</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-the-cusp-of-big-data</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1261249967]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
							</item>
		<item>
		<title>Oh, Oracle</title>
		<link>http://mattthinksso.wordpress.com/2010/03/05/oh-oracle/</link>
		<pubDate>Fri, 05 Mar 2010 17:44:26 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Big Data]]></category>

		<category domain="category" nicename="big-data"><![CDATA[Big Data]]></category>

		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=23</guid>
		<description></description>
		<content:encoded><![CDATA[So, I was responsible for a pretty unfortunate bug today — no way around it, I messed up.  It was classic — there was a "TODO" block where I meant to come back and finish some code, and no doubt got distracted by some very valid crisis.  Fortunately, it was caught before it affected production data,  but it was in test, and visible, and it was scary that it had gotten that far.

But I couldn't help but be bitter about how that block of code came to be in the first place:  the code that contained the bug was part of an elaborate scheme designed to work around joining to a particularly large table in certain circumstances. 

Now, it’s big data…we have to do what we can for efficiency.  But it made me slightly bitter that the more we optimize ofr relational databases, trying to eke more and more performance out of Oracle, the farther we move from a clear data model, and the less “relational” we’re using our RDBMS for.   We go through contortions, and these introduce bugs. 

I’m not sure who that might be a lesson for, but perhaps if anyone is wondering the tradeoffs of  using a tried-and-true RDBMS to avoid bugs in newer systems, whether MapReduce or a “NoSQL” store, perhaps it’s worth noting that there is some tradeoff of bugs in the data store in question vs. bugs due to the increased complexity of your own code introduced as you contort your data model for performance’s sake. 
 ]]></content:encoded>
		<excerpt:encoded><![CDATA[So, I was responsible for a pretty unfortunate bug today — no way around it, I messed up.  It was classic — there was a "TODO" block where I meant to come back and finish some code, and no doubt got distracted by some very valid crisis.  Fortunately, it was caught before it affected production data,  but it was in test, and visible, and it was scary that it had gotten that far.

But I couldn't help but be bitter about how that block of code came to be in the first place...]]></excerpt:encoded>
		<wp:post_id>23</wp:post_id>
		<wp:post_date>2010-03-05 12:44:26</wp:post_date>
		<wp:post_date_gmt>2010-03-05 17:44:26</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>oh-oracle</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1267817758]]></wp:meta_value>
		</wp:postmeta>
							</item>
		<item>
		<title>On Configuration</title>
		<link>http://mattthinksso.wordpress.com/2010/12/10/on-configuration/</link>
		<pubDate>Fri, 10 Dec 2010 21:07:42 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Programming]]></category>

		<category domain="category" nicename="programming"><![CDATA[Programming]]></category>

		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=38</guid>
		<description></description>
		<content:encoded><![CDATA[I've been musing recently on how to scale our configuration system up and out: making it handle single nodes better, and handle multiple nodes...well, at all.
<h2>How it works now</h2>
Right now, our configuration is spread across the following places, roughly in order of precedence:
<ul>
	<li>System properties (-D properties set on the command line)</li>
	<li>a user-editable property file, for overrides to default properties.  This is preserved during system upgrades.</li>
	<li>a "system defaults" property file, user-visible but overwritten during upgrades.</li>
	<li>a database table:  for a while, we were standardizing on properties in the database.  More on this later...</li>
	<li>some property files buried inside jars, which hold some database queries, some default values, and so on.</li>
	<li>log4j.xml</li>
</ul>
We use a MergedPropertyPlaceholderConfigurer, along with a couple other configuration classes available <a title="blast-config on github" href="https://github.com/matthoffman/blast-config">on github</a>) that merges properties from most of the above locations together, ranks them in order of precedence, and then sets them at startup time using Spring's standard placeholder syntax (${property.name}). Database properties are loaded from a table using a special Spring property loader.

So any property can be set in a higher-precedence location and it will override one set in a lower-precedence location.

In practice, new properties tend to get set in the property file. Why? Because database changes require a patch (like a migration in the Rails world) which needs to get migrated to each applicable environment.  Deploying the code to a development or test server then requires both a code update and a database update. In practice, the dependency between the code and particular database patches is a bit of a hassle -- certainly far more so than just adding it to a property file which gets deployed along with the code.  A bad motivation for keeping properties in files?  Perhaps... but it is the reality of it. A system that raises the barrier of entry for doing "the right thing" is a bad system.  Which brings us to...
<h2>Problems with current system</h2>
<ol>
	<li>property files are cumbersome in a distributed environment.  Many of our deployments are single-node, but more and more they're distributed, and distributed deployments should be our default going forward.</li>
	<li>For properties stored in the DB, adding, removing or updating any property requires a DB task, and then a DB refresh, which has the effect of discouraging parameterizing things. You tend to think, "eh, well... I'll just hard-code this for now..."</li>
	<li>Properties are loaded at startup time only – you can't change property values without changing each property file and then restarting each node.</li>
</ol>
<h2>Requirements for a new system:</h2>
I'd like to borrow requirements from <a href="http://jim-mcbeath.blogspot.com/2010/01/reload-that-config-file.html">http://jim-mcbeath.blogspot.com/2010/01/reload-that-config-file.html</a>:
<ol>
<li>Reloading a configuration should be a simple operation for the operator to trigger.
<li>It should not be possible to load an invalid configuration. If the operator tries to do so, the application should continue running with the old configuration.
<li>When reloading a configuration, the application should smoothly switch from the old configuration to the new configuration, ensuring that it is always operating with a consistent configuration. More precisely, an operational sequence that requires a consistent set of configuration parameters for the entire sequence should complete its sequence with the same set of configuration parameters as were active when the sequence started. – For us, this is actually pretty easy.  Our app depends on a task distribution framework, meaning that work is defined as a series of tasks with defined beginnings and endings.  So, we merely need to load the configuration at the beginning of each discrete unit of work.
<li>The application should provide feedback so that the operator knows what the application is doing. Logging, notification or statistics about configuration reloads should be available.

<br /><br />...and I'd add:
<li>We should be able to set configurations for all nodes at once (this could mean using the database, or perhaps a command-line tool that sprays configurations out to the various nodes, plus a web service to tell nodes to reload..or something else entirely).
<li>We should be able to view the current configuration for each node easily.
<li>We should be able to share configuration between our app and other related applications, again, this could be database, or a web service that exposes our properties to other applications.
</ol>
<h2>Current thoughts</h2>
At the code level, I'm thinking of loading properties at the beginning of each task, using a base class or something built into the framework.
Reloading and interrogating the configuration could be via a web service (get_configuration / set_configuration). For requirement 3, the easiest option seems to be to use Configgy as a configuration base.
As far as centralized configuration goes, I'm up in the air. Some options:
<ul>
	<li>Spraying config files (scp'ing configuration files to each server, which would have to be tied to either an automatic poll of files, or a manual "reload_configuration" web service call)</li>
	<li>distributing configuration using a web service (node 2 calls get_all_configuration on node 1, and sets its own configuration accordingly) – but it would need to be saved somewhere in case node 2 restarts when node 1 isn't available. The database is an option, but has development-time issues as noted above.</li>
	<li>saving all configuration in Zookeeper.</li>
</ul>
What i'd really like, though, is a configuration system that kept properties in an immutable data structure that kept track of where properties came from -- so, I could define the locations properties should come from, and then in the application I could say,  "config.getProperty('foo')"  and get the value with the highest precedence (whether that's from an override file, a database table, or whatever).  But I could also say "config.getPropertyDetails('foo') " and get a list that said "property 'foo' is set to 'bar' by local override, is set to 'groo' by the central configuration server, and the 'moo' as a fallback default." Now, why do I want that? Mainly for on-site debugging:  "I set the property in this property file, but it's not working!"    

<h2>Some related (external) links:</h2>
<ul>
	<li><a href="http://soupinadeli.com/resilient-software-configuration/">http://soupinadeli.com/resilient-software-configuration/</a> - a good article about how a configuration system should work, and why.</li>
	<li><a href="http://jim-mcbeath.blogspot.com/2010/01/reload-that-config-file.html">http://jim-mcbeath.blogspot.com/2010/01/reload-that-config-file.html</a> - another good article, cited above.</li>
        <li><a href="http://stackoverflow.com/questions/1244455/where-how-to-store-distributed-configuration-data">http://stackoverflow.com/questions/1244455/where-how-to-store-distributed-configuration-data</a> - an interesting solution for all-database configuration, with a "central" table and a "local" table (for "defaults" and "overrides", like what I'm doing currently for property files).   The answerer takes it a couple steps farther, with an interesting SQL analytics query to pull it all in at once. 
        <li><a href="http://commons.apache.org/configuration/">Apache Commons Configuration</a> comes close to the model I described above, with their <a href="http://commons.apache.org/configuration/apidocs/org/apache/commons/configuration/CompositeConfiguration.html">Composite Configuration</a> class.  I looked at Commons Configuration a year or so ago, and thought it was interesting but not quite what I was looking for (and their Hierarchical Configuration concepts can get pretty hairy). But I'm intrigued by the CompositeConfiguration class, so I need to look into it again.  
Of course, the project is all but dead -- last updated 2008 -- but how often does a configuration library really need to change?  
        <li><a href="https://issues.apache.org/jira/browse/CONFIGURATION-395">Interesting patch</a> to Commons Configuration for Groovy interpolation (i.e. put Groovy in property values, to be evaluated at load-time)
        <li><a href=""></a>...more to come
</ul>

I'm open to ideas, as well...  Anyone have best-practices for distributed configuration?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>38</wp:post_id>
		<wp:post_date>2010-12-10 16:07:42</wp:post_date>
		<wp:post_date_gmt>2010-12-10 21:07:42</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>on-configuration</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1296353135:11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>jabber_published</wp:meta_key>
		<wp:meta_value><![CDATA[1296337876]]></wp:meta_value>
		</wp:postmeta>
							</item>
		<item>
		<title>Cluster Management and Task Distribution: Zookeeper vs. JGroups</title>
		<link>http://mattthinksso.wordpress.com/2011/01/29/cluster-management-and-task-distribution-zookeeper-vs-jgroups/</link>
		<pubDate>Sat, 29 Jan 2011 21:02:45 +0000</pubDate>
		<dc:creator><![CDATA[matt]]></dc:creator>
		
		<category><![CDATA[Big Data]]></category>

		<category domain="category" nicename="big-data"><![CDATA[Big Data]]></category>

		<category><![CDATA[Programming]]></category>

		<category domain="category" nicename="programming"><![CDATA[Programming]]></category>

		<guid isPermaLink="false">http://mattthinksso.wordpress.com/?p=27</guid>
		<description></description>
		<content:encoded><![CDATA[<div>
<h2 id="internal-source-marker_0.6950423333328217">The question</h2>
Which technology stack makes more sense for a distributed task-management framework?
<h2>The backstory</h2>
The target app I’m working on at the moment is a task-management framework: it distributes discrete units of work (tasks) to workers which may be spread across multiple servers.  Right now, it’s a single-master system: we can add as many nodes as we want (at runtime, if we want) but only one can ever serve as the “master” node, and that node can’t fail.  The master needs to keep track of a bunch of tasks, which have a hierarchical structure (tasks can have one parent and 0-n child tasks).  It manages the tasks' interdependencies, checks on the health of the worker nodes, and redistributes tasks if they time out or if their worker node goes down.
A lot of task-management systems put tasks in one or more queues (Resque, beanstalkd, etc.); that’s what we did as well, in our first version.  And this works well; you can model a lot of problem domains this way.  But our task data model is a bit more complex than that, though: tasks can have children, and can define whether those children should be executed in serial or in parallel.  So you can define that task A must execute before task B, but then tasks C through Z can execute in parallel.  When Task B executes, it can decide to split its workload up into 100 smaller tasks, B1 through B100, which all execute in parallel, and would all execute before C starts.  So the tasks end up looking more like a tree than a queue...modeling them as a queue gets a bit convoluted.  You could argue that the best answer would be to re-model our tasks to fit within Resque, beanstalkd, or the like, but at this point that'd actually be more work. And you can't deny this is fun stuff to think about...

So the single point of failure needs to go.  I’m looking at some of the options for instead distributing the data (the tree of tasks, who's executing what, whether the nodes are healthy, whether a task has timed out) across multiple servers.
<h2>The requirements</h2>
<ol>
	<li>Consistency: the task tree should be shared between more than one server, with guaranteed consistency.  I.e if server A is serving as the master node and it goes down, another server needs to be able to pick up and continue, with the exact same view of the current task tree.  We don’t want tasks to accidentally get dropped or executed twice because server B’s version of the task tree wasn't up-to-date.</li>
	<li>Elasticity: nodes have to be able to come online and participate in the cluster, and then drop off again.  The full list of nodes can’t be hard-coded at startup.</li>
	<li>Partition-tolerance: Ideally, we should be able to handle a network partition.  We can do that at the expense of availability: that is, if there are 5 nodes, numbered 1 through 5, and nodes 1 and 2 get separated from 3 through 5, it’s OK for 1 and 2 to realize they’re the minority and stop processing.   The following caveats apply:
<ol>
	<li>We need a way to handle an intentional reduction in servers.  That is, if we spun up 10 extra cloud servers for a few hours to help in peak processing, we’d need a way to tell the cluster that we were going to spin them back down again, so the remaining nodes didn’t think they were a minority segment of a partitioned network.</li>
	<li>In case of a catastrophic failure of a majority of servers, I’d like a way to kick the remaining nodes and tell them to process anyway.  But I’m ok if that requires manual intervention... that’s an extreme case.</li>
</ol>
</li>
	<li>Scalability: I’m targeting 100 nodes as a practical maximum.  In production, we’ve used up to 5 nodes.  I don’t expect us to go above 10 in the near future, so I’m adding an order-of-magnitude safety margin on top of that.  Of course, when the task management framework goes open source, 100 nodes will be quite feasible for users with different problem domains and deployment patterns.  In our typical deployment scenarios, customers prefer fewer, larger servers.</li>
	<li>EC2:  The clustering option has to be able to run on EC2 instances.  It can't depend on IP multicast.</li>
	<li>Deployment simplicity:   As few moving parts as possible, to make integrating and deploying the software as simple as possible. Our app is often deployed by the client on their hardware, so we'd like to make their lives as easy as possible. And as the framework goes open source, of course, it's going to be more helpful for a more people if it has a low barrier of entry.</li>
	<li>Implementation simplicity:  The less code I write, the fewer bugs I introduce.</li>
	<li>Event bus:  nodes can also pass events to other nodes, typically by multicast.  We use JMS for this currently, and we can continue to do so, but it’d be nice to get rid of that particular moving part if possible.</li>
</ol>
There is not necessarily a requirement to persist all tasks to disk, in order to keep them safe between server restarts.  In our previous version, tasks were kept in a JMS queue, and we ended up turning disk persistence off -- in practice, in the case of a failure that requires manual intervention (for example, a JMS broker failure, or the failure of all nodes that had tasks in memory) almost always means that we want to manually restart jobs after the system comes back up -- possibly different jobs, possibly a reduced set of tasks to make up time.  We found that we rarely want to start back up exactly where we left off.
So, if the solution automatically persists to disk, I’ll probably create a way to bypass that if necessary (a command-line flag that lets us clear the current tasks on startup, perhaps).
<h2>Zookeeper</h2>
<table><col width="*"></col> <col width="*"></col> 
<tbody>
<tr>
<td>Consistency</td>
<td>Zookeeper pushes everything to disk -- writes are always persisted, for safety.
Zookeeper is designed for cluster coordination, and it’s well-thought-out.  It has documented consistency guarantees, lots of documented recipes for things like leader election, and supports subscription of listeners on tree changes.</td>
</tr>
<tr>
<td>Elasticity</td>
<td>We’d have to pick three servers up front that would serve as Zookeeper nodes as well as normal nodes. Those nodes woulnd’t be elastic, but other nodes could come up and down without a problem.</td>
</tr>
<tr>
<td>Partition-tolerance</td>
<td>The “live” partition would be defined as “the partition that can still connect with the Zookeeper cluster”.   I have to check on how Zookeeper handles network partitions; I believe it’s just a quorum-based algorithm (if one node is separated from the other two, it stops processing).</td>
</tr>
<tr>
<td>Scalability</td>
<td>Three Zookeeper nodes could support 100 nodes without a problem, according to anecdotal experience.  At that point, we would have the option of moving the Zookeeper nodes onto dedicated hardware. Having that option is the upside to Zookeeper’s deployment complexity: you can separate them, if there’s a need.</td>
</tr>
<tr>
<td>EC2-friendliness</td>
<td>Zookeeper uses TCP; it’s been used on EC2 before.</td>
</tr>
<tr>
<td>Deployment Simplicity</td>
<td>The main downside for Zookeeper is its operational requirements:  two classes of servers (with Zookeeper and without), potentially separate server instances running on Zookeeper-enabled servers, fast disk access required (according to the ZK operations manual, two fast disks -- one for logs, the other for … er..something else.)  That’s a significant increase in operational complexity for a product that is distributed to and maintained by clients.  That also means work necessary for us to make it as turnkey as possible.A Zookeeper-based solution would pick three or five nodes to be Zookeeper nodes as well as task nodes.  Ideally, for Zookeeper’s purposes, it’d be three dedicated servers, but that isn’t going to happen in our architecture.  So Zookeeper will have to coexist with the current app on servers where it’s installed.  For deployment simplicity, I’ll probably need to come up with a way to start up Zookeeper automatically when the server comes up on those nodes. Zookeeper used to not play well as an embedded app (according to some reports from the Katta project); that may be fixed now, but if not I may need to put that logic in the bootstrap script.</td>
</tr>
<tr>
<td>Implementation Simplicity</td>
<td>Zookeeper would provide a single, shared view of the task tree.  The system would be up as long as the majority of Zookeeper nodes remained up; any node could be leader, as long as they could connect to Zookeeper to view and update the shared state of tasks.   Other nodes could potentially go straight to Zookeeper to query the task tree, if we were comfortable with losing that layer of abstraction.  Either way, it would be simple implementation-wise.</td>
</tr>
<tr>
<td>Event bus</td>
<td>Zookeeper doesn’t provide events...unless you write events directly to Zookeeper.  It could, however, obviate the need for a lot of events.  For example, we wouldn’t need a “task finished” event if all nodes just set watches on tasks...they’d be automatically notified.  Same with hearbeats, node failure, and so on -- they’d be taken care of by Zookeeper.</td>
</tr>
</tbody>
</table>
Zookeeper doesn’t list AIX as a supported environment, which is interesting.  We do have to support AIX (unfortunately).
<h2>JGroups</h2>
<table><col width="*"></col> <col width="*"></col> 
<tbody>
<tr>
<td>Consistency</td>
<td>JGroups is a lot more of a DIY solution, although it supports guaranteed message delivery and cluster management out-of-the-box.
With guaranteed message delivery, we could have consistent in-memory views of the task tree without too much effort; we wouldn’t need to write something that also persisted to disk.</td>
</tr>
<tr>
<td>Elasticity</td>
<td>I have to look into how elastic JGroups can be without IP multicast.  I know it can work without IP multicast; I just don’t know exactly HOW it works.</td>
</tr>
<tr>
<td>Partition-tolerance</td>
<td>Good question....</td>
</tr>
<tr>
<td>Scalability</td>
<td>More speculative, because JGroups has the option for infinite combinations of protocols on its stack.  Worst case, the cluster needs to define two classes of nodes, similar to the Zookeeper implementation: “nodes which can become leaders” and “nodes which can’t.”</td>
</tr>
<tr>
<td>EC2-friendliness</td>
<td>JGroups can be used on EC2 as long as you use TCP-based “multicast” instead of actual IP multicast.</td>
</tr>
<tr>
<td>Deployment Simplicity</td>
<td>JGroups would be baked in; worst case, you need to define a couple properties per server (IP of one or more nodes to connect to, and perhaps something like “is this a temporary node”)</td>
</tr>
<tr>
<td>Implementation Simplicity</td>
<td>More DIY.  The basic protocol stack is provided, along with clustering, leader election, “state transfer” (to bootstrap a new node with a current picture of the task tree) and the lower level; we’d just have to tie it together and fill in the gaps for application-specific needs.</td>
</tr>
<tr>
<td>Event bus</td>
<td>JGroups can also give us message passing, taking the place of a simple JMS topic.  I have a lot of questions here, though:  how does this handle nodes dropping?  Does it buffer events until the node comes back on?  If so, how do we handle nodes that permanently drop?</td>
</tr>
</tbody>
</table>
</div>
<h2>Conclusion</h2>
...tbd...]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>27</wp:post_id>
		<wp:post_date>2011-01-29 16:02:45</wp:post_date>
		<wp:post_date_gmt>2011-01-29 21:02:45</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>cluster-management-and-task-distribution-zookeeper-vs-jgroups</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
								<wp:postmeta>
		<wp:meta_key>_edit_last</wp:meta_key>
		<wp:meta_value><![CDATA[11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>_edit_lock</wp:meta_key>
		<wp:meta_value><![CDATA[1296334973:11329143]]></wp:meta_value>
		</wp:postmeta>
				<wp:postmeta>
		<wp:meta_key>jabber_published</wp:meta_key>
		<wp:meta_value><![CDATA[1296334968]]></wp:meta_value>
		</wp:postmeta>
							</item>
	</channel>
</rss>
